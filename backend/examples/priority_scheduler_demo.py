#!/usr/bin/env python3
"""
ä¼˜å…ˆçº§è°ƒåº¦ç®—æ³•æ¼”ç¤ºè„šæœ¬

å±•ç¤ºåŸºäºä¼˜å…ˆçº§çš„èµ„æºåˆ†é…å’ŒæŠ¢å æœºåˆ¶çš„å·¥ä½œåŸç†
"""
import asyncio
import logging
import sys
import os
from datetime import datetime
from typing import List

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

from backend.app.services.resource_scheduler import PriorityResourceScheduler
from backend.app.models.schemas import (
    ModelConfig, GPUInfo, ResourceRequirement
)
from backend.app.models.enums import (
    FrameworkType, ModelStatus, GPUVendor, ScheduleResult
)


class SchedulerDemo:
    """è°ƒåº¦å™¨æ¼”ç¤ºç±»"""
    
    def __init__(self):
        self.scheduler = PriorityResourceScheduler()
        self.logger = logging.getLogger(__name__)
    
    def create_sample_models(self) -> List[ModelConfig]:
        """åˆ›å»ºç¤ºä¾‹æ¨¡å‹é…ç½®"""
        models = [
            ModelConfig(
                id="critical_model",
                name="å…³é”®ä¸šåŠ¡æ¨¡å‹",
                framework=FrameworkType.VLLM,
                model_path="/models/critical_llama_70b.safetensors",
                priority=10,  # æœ€é«˜ä¼˜å…ˆçº§
                gpu_devices=[0, 1],
                parameters={
                    "model_size_gb": 70.0,
                    "precision": "fp16",
                    "max_seq_len": 4096,
                    "tensor_parallel_size": 2
                },
                resource_requirements=ResourceRequirement(
                    gpu_memory=40960,  # 40GB
                    gpu_devices=[0, 1]
                )
            ),
            ModelConfig(
                id="production_model",
                name="ç”Ÿäº§ç¯å¢ƒæ¨¡å‹",
                framework=FrameworkType.LLAMA_CPP,
                model_path="/models/production_llama_13b.gguf",
                priority=8,  # é«˜ä¼˜å…ˆçº§
                gpu_devices=[0],
                parameters={
                    "model_size_gb": 13.0,
                    "precision": "fp16",
                    "n_ctx": 2048
                },
                resource_requirements=ResourceRequirement(
                    gpu_memory=16384,  # 16GB
                    gpu_devices=[0]
                )
            ),
            ModelConfig(
                id="dev_model",
                name="å¼€å‘æµ‹è¯•æ¨¡å‹",
                framework=FrameworkType.LLAMA_CPP,
                model_path="/models/dev_llama_7b.gguf",
                priority=5,  # ä¸­ç­‰ä¼˜å…ˆçº§
                gpu_devices=[1],
                parameters={
                    "model_size_gb": 7.0,
                    "precision": "fp16",
                    "n_ctx": 2048
                },
                resource_requirements=ResourceRequirement(
                    gpu_memory=10240,  # 10GB
                    gpu_devices=[1]
                )
            ),
            ModelConfig(
                id="experimental_model",
                name="å®éªŒæ€§æ¨¡å‹",
                framework=FrameworkType.VLLM,
                model_path="/models/experimental_model",
                priority=3,  # è¾ƒä½ä¼˜å…ˆçº§
                gpu_devices=[0],
                parameters={
                    "model_size_gb": 30.0,
                    "precision": "int8",
                    "max_seq_len": 1024
                },
                resource_requirements=ResourceRequirement(
                    gpu_memory=12288,  # 12GB
                    gpu_devices=[0]
                )
            ),
            ModelConfig(
                id="background_model",
                name="åå°å¤„ç†æ¨¡å‹",
                framework=FrameworkType.LLAMA_CPP,
                model_path="/models/background_model.gguf",
                priority=1,  # æœ€ä½ä¼˜å…ˆçº§
                gpu_devices=[1],
                parameters={
                    "model_size_gb": 3.0,
                    "precision": "int4",
                    "n_ctx": 512
                },
                resource_requirements=ResourceRequirement(
                    gpu_memory=4096,  # 4GB
                    gpu_devices=[1]
                )
            )
        ]
        return models
    
    def create_sample_gpu_info(self) -> List[GPUInfo]:
        """åˆ›å»ºç¤ºä¾‹GPUä¿¡æ¯"""
        return [
            GPUInfo(
                device_id=0,
                name="NVIDIA RTX 4090",
                vendor=GPUVendor.NVIDIA,
                memory_total=24576,  # 24GB
                memory_used=0,
                memory_free=24576,
                utilization=0.0,
                temperature=45.0,
                power_usage=50.0,
                driver_version="535.86.10"
            ),
            GPUInfo(
                device_id=1,
                name="NVIDIA RTX 4090",
                vendor=GPUVendor.NVIDIA,
                memory_total=24576,  # 24GB
                memory_used=0,
                memory_free=24576,
                utilization=0.0,
                temperature=42.0,
                power_usage=45.0,
                driver_version="535.86.10"
            )
        ]
    
    def print_gpu_status(self, gpu_info: List[GPUInfo], title: str = "GPUçŠ¶æ€"):
        """æ‰“å°GPUçŠ¶æ€"""
        print(f"\n=== {title} ===")
        for gpu in gpu_info:
            usage_percent = (gpu.memory_used / gpu.memory_total) * 100
            print(f"GPU {gpu.device_id}: {gpu.name}")
            print(f"  å†…å­˜: {gpu.memory_used}MB / {gpu.memory_total}MB ({usage_percent:.1f}%)")
            print(f"  å¯ç”¨: {gpu.memory_free}MB")
            print(f"  åˆ©ç”¨ç‡: {gpu.utilization:.1f}%")
            print(f"  æ¸©åº¦: {gpu.temperature:.1f}Â°C")
    
    def print_model_status(self, title: str = "æ¨¡å‹çŠ¶æ€"):
        """æ‰“å°æ¨¡å‹çŠ¶æ€"""
        print(f"\n=== {title} ===")
        model_states = self.scheduler.get_model_states()
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        sorted_models = sorted(
            model_states.items(),
            key=lambda x: x[1].config.priority,
            reverse=True
        )
        
        for model_id, state in sorted_models:
            status_icon = {
                ModelStatus.STOPPED: "â¹ï¸",
                ModelStatus.STARTING: "ğŸ”„",
                ModelStatus.RUNNING: "âœ…",
                ModelStatus.ERROR: "âŒ",
                ModelStatus.STOPPING: "â¸ï¸",
                ModelStatus.PREEMPTED: "âš ï¸"
            }.get(state.status, "â“")
            
            print(f"{status_icon} {state.config.name} (ä¼˜å…ˆçº§: {state.config.priority})")
            print(f"   çŠ¶æ€: {state.status.value}")
            if state.allocated_resources:
                print(f"   GPU: {state.allocated_resources.gpu_devices}")
                print(f"   å†…å­˜: {state.allocated_resources.memory_allocated}MB")
            if state.preemption_count > 0:
                print(f"   è¢«æŠ¢å æ¬¡æ•°: {state.preemption_count}")
    
    def print_schedule_history(self, limit: int = 5):
        """æ‰“å°è°ƒåº¦å†å²"""
        print(f"\n=== æœ€è¿‘{limit}æ¬¡è°ƒåº¦å†³ç­– ===")
        history = self.scheduler.get_schedule_history(limit)
        
        for i, decision in enumerate(reversed(history), 1):
            result_icon = {
                ScheduleResult.SUCCESS: "âœ…",
                ScheduleResult.INSUFFICIENT_RESOURCES: "âŒ",
                ScheduleResult.PREEMPTION_REQUIRED: "âš ï¸",
                ScheduleResult.FAILED: "ğŸ’¥"
            }.get(decision.result, "â“")
            
            print(f"{i}. {result_icon} æ¨¡å‹: {decision.model_id}")
            print(f"   æ—¶é—´: {decision.decision_time.strftime('%H:%M:%S')}")
            print(f"   ç»“æœ: {decision.result.value}")
            print(f"   åŸå› : {decision.reason}")
            if decision.preempted_models:
                print(f"   æŠ¢å æ¨¡å‹: {decision.preempted_models}")
    
    async def simulate_gpu_usage(self, gpu_info: List[GPUInfo], model_states: dict):
        """æ¨¡æ‹ŸGPUä½¿ç”¨æƒ…å†µ"""
        for gpu in gpu_info:
            gpu.memory_used = 0
            gpu.memory_free = gpu.memory_total
            gpu.utilization = 0.0
        
        # æ ¹æ®æ¨¡å‹åˆ†é…æƒ…å†µæ›´æ–°GPUçŠ¶æ€
        for model_id, state in model_states.items():
            if state.status == ModelStatus.RUNNING and state.allocated_resources:
                for gpu_id in state.allocated_resources.gpu_devices:
                    gpu = next((g for g in gpu_info if g.device_id == gpu_id), None)
                    if gpu:
                        # ç®€åŒ–è®¡ç®—ï¼šå‡è®¾å†…å­˜å¹³å‡åˆ†é…åˆ°æ‰€æœ‰GPU
                        memory_per_gpu = state.allocated_resources.memory_allocated // len(state.allocated_resources.gpu_devices)
                        gpu.memory_used += memory_per_gpu
                        gpu.memory_free = gpu.memory_total - gpu.memory_used
                        gpu.utilization = min(100.0, (gpu.memory_used / gpu.memory_total) * 100)
                        gpu.temperature += gpu.utilization * 0.3  # æ¨¡æ‹Ÿæ¸©åº¦ä¸Šå‡
    
    async def run_demo(self):
        """è¿è¡Œæ¼”ç¤º"""
        print("ğŸš€ ä¼˜å…ˆçº§è°ƒåº¦ç®—æ³•æ¼”ç¤ºå¼€å§‹")
        print("=" * 60)
        
        # åˆ›å»ºç¤ºä¾‹æ•°æ®
        models = self.create_sample_models()
        gpu_info = self.create_sample_gpu_info()
        
        # æ³¨å†Œæ‰€æœ‰æ¨¡å‹
        print("\nğŸ“ æ³¨å†Œæ¨¡å‹åˆ°è°ƒåº¦å™¨...")
        for model in models:
            self.scheduler.register_model(model)
        
        self.print_model_status("åˆå§‹æ¨¡å‹çŠ¶æ€")
        self.print_gpu_status(gpu_info, "åˆå§‹GPUçŠ¶æ€")
        
        # æ¨¡æ‹ŸGPUç›‘æ§å™¨
        async def mock_get_gpu_info():
            await self.simulate_gpu_usage(gpu_info, self.scheduler.get_model_states())
            return gpu_info
        
        # æ›¿æ¢GPUç›‘æ§å™¨
        import backend.app.services.resource_scheduler as scheduler_module
        scheduler_module.gpu_monitor.get_gpu_info = mock_get_gpu_info
        
        # åœºæ™¯1: æŒ‰ä¼˜å…ˆçº§é¡ºåºå¯åŠ¨æ¨¡å‹
        print("\n" + "=" * 60)
        print("ğŸ“‹ åœºæ™¯1: æŒ‰ä¼˜å…ˆçº§é¡ºåºå¯åŠ¨æ¨¡å‹")
        print("=" * 60)
        
        # å…ˆå¯åŠ¨ä½ä¼˜å…ˆçº§æ¨¡å‹
        low_priority_models = ["background_model", "experimental_model", "dev_model"]
        
        for model_id in low_priority_models:
            print(f"\nğŸ”„ å¯åŠ¨æ¨¡å‹: {model_id}")
            self.scheduler.update_model_status(model_id, ModelStatus.RUNNING)
            
            # æ¨¡æ‹Ÿèµ„æºåˆ†é…
            model_state = self.scheduler._model_states[model_id]
            from backend.app.models.schemas import ResourceAllocation
            model_state.allocated_resources = ResourceAllocation(
                gpu_devices=model_state.config.resource_requirements.gpu_devices,
                memory_allocated=model_state.config.resource_requirements.gpu_memory,
                allocation_time=datetime.now()
            )
            
            await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿå¯åŠ¨æ—¶é—´
            
            self.print_model_status()
            self.print_gpu_status(gpu_info)
        
        # åœºæ™¯2: é«˜ä¼˜å…ˆçº§æ¨¡å‹è¯·æ±‚èµ„æºï¼Œè§¦å‘æŠ¢å 
        print("\n" + "=" * 60)
        print("ğŸ“‹ åœºæ™¯2: é«˜ä¼˜å…ˆçº§æ¨¡å‹è§¦å‘æŠ¢å ")
        print("=" * 60)
        
        print(f"\nğŸ”¥ å…³é”®ä¸šåŠ¡æ¨¡å‹è¯·æ±‚èµ„æº (ä¼˜å…ˆçº§: 10)")
        result = await self.scheduler.schedule_model("critical_model")
        
        print(f"è°ƒåº¦ç»“æœ: {result.value}")
        
        self.print_model_status("æŠ¢å åæ¨¡å‹çŠ¶æ€")
        self.print_gpu_status(gpu_info, "æŠ¢å åGPUçŠ¶æ€")
        self.print_schedule_history()
        
        # åœºæ™¯3: æ˜¾ç¤ºæŠ¢å ç»Ÿè®¡
        print("\n" + "=" * 60)
        print("ğŸ“Š æŠ¢å ç»Ÿè®¡ä¿¡æ¯")
        print("=" * 60)
        
        stats = self.scheduler.get_preemption_stats()
        print(f"è¿‡å»1å°æ—¶æŠ¢å æ¬¡æ•°: {stats['total_preemptions_last_hour']}")
        print(f"è¿‡å»24å°æ—¶æŠ¢å æ¬¡æ•°: {stats['total_preemptions_last_day']}")
        print(f"æŠ¢å é¢‘ç‡é™åˆ¶: {stats['preemption_rate_limit']}/å°æ—¶")
        
        print("\næ¨¡å‹è¢«æŠ¢å æ¬¡æ•°ç»Ÿè®¡:")
        for model_id, count in stats['model_preemption_counts'].items():
            if count > 0:
                model_name = self.scheduler._model_states[model_id].config.name
                print(f"  {model_name}: {count}æ¬¡")
        
        # åœºæ™¯4: èµ„æºé‡Šæ”¾åçš„è‡ªåŠ¨æ¢å¤
        print("\n" + "=" * 60)
        print("ğŸ“‹ åœºæ™¯4: èµ„æºé‡Šæ”¾åçš„æ¨¡å‹æ¢å¤")
        print("=" * 60)
        
        print("ğŸ”„ å…³é”®ä¸šåŠ¡æ¨¡å‹å®Œæˆä»»åŠ¡ï¼Œé‡Šæ”¾èµ„æº...")
        self.scheduler.update_model_status("critical_model", ModelStatus.STOPPED)
        await self.scheduler._release_resources("critical_model")
        
        # å°è¯•æ¢å¤è¢«æŠ¢å çš„æ¨¡å‹
        print("\nğŸ”„ å°è¯•æ¢å¤è¢«æŠ¢å çš„æ¨¡å‹...")
        for model_id, state in self.scheduler.get_model_states().items():
            if state.status == ModelStatus.PREEMPTED:
                print(f"æ¢å¤æ¨¡å‹: {state.config.name}")
                result = await self.scheduler.schedule_model(model_id)
                print(f"æ¢å¤ç»“æœ: {result.value}")
        
        self.print_model_status("æ¢å¤åæ¨¡å‹çŠ¶æ€")
        self.print_gpu_status(gpu_info, "æ¢å¤åGPUçŠ¶æ€")
        
        print("\n" + "=" * 60)
        print("âœ… æ¼”ç¤ºå®Œæˆï¼")
        print("=" * 60)
        
        # æœ€ç»ˆç»Ÿè®¡
        final_stats = self.scheduler.get_preemption_stats()
        print(f"\nğŸ“ˆ æœ€ç»ˆç»Ÿè®¡:")
        print(f"  æ€»è°ƒåº¦å†³ç­–: {len(self.scheduler.get_schedule_history())}")
        print(f"  æ€»æŠ¢å æ¬¡æ•°: {sum(final_stats['model_preemption_counts'].values())}")
        print(f"  æ´»è·ƒæ¨¡å‹æ•°: {sum(1 for s in self.scheduler.get_model_states().values() if s.status == ModelStatus.RUNNING)}")


async def main():
    """ä¸»å‡½æ•°"""
    demo = SchedulerDemo()
    await demo.run_demo()


if __name__ == "__main__":
    asyncio.run(main())