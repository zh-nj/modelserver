---
inclusion: always
---

# 产品概述

## 大语言模型推理服务

一个全面的大语言模型(LLM)推理管理和监控服务，支持多种推理框架和GPU设备。系统提供智能资源调度、自动模型管理和基于Web的AI模型部署管理功能。

### 核心价值主张
- **多框架支持**: 管理运行在llama.cpp、vLLM和Docker容器上的模型
- **智能资源调度**: 基于优先级的自动资源分配和抢占机制
- **实时监控**: 全面的GPU利用率、模型健康状态和性能跟踪
- **Web管理界面**: 用户友好的配置和监控仪表板
- **高可用性**: 自动故障转移、恢复和持久化配置

### 目标用户
- 管理AI基础设施的系统管理员
- 部署LLM服务的DevOps团队
- 集成托管AI模型的开发者

### 核心功能
- 自动GPU资源检测和分配
- 基于优先级的模型调度和抢占
- 实时系统监控和告警
- 编程访问的RESTful API
- 基于Web的配置和管理界面
- 持久化配置和灾难恢复

### 业务场景
- **多模型并发**: 在有限GPU资源上同时运行多个AI模型
- **动态负载管理**: 根据业务优先级自动调整资源分配
- **运维监控**: 实时掌握系统状态，快速响应异常情况
- **开发集成**: 为应用程序提供稳定的AI推理服务接口

### 技术优势
- 支持主流推理框架，降低迁移成本
- 智能调度算法，最大化资源利用率
- 完善的监控体系，保障服务稳定性
- 模块化架构，便于扩展和维护